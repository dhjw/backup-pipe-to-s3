#!/bin/bash
# archive and encrypt backup with pipe to s3
# backup.cfg should be in the same folder as this script
# recommend a daily cron task for root user, e.g. 0 0 * * * /usr/local/bin/backup > /var/log/backup.log

echo "start $(date)"

# read config
if [ ! -f "$(dirname "$0")/backup.cfg" ] || [ ! -r "$(dirname "$0")/backup.cfg" ]; then echo "error reading $(dirname "$0")/backup.cfg"; exit; fi
source $(dirname "$0")/backup.cfg

# function: exit with error, email admin
function ex () {
	echo -e "error: $1" #| sed -z '$ s/\n$//'
	echo "sending email to $email"
	echo -e "$1" | mail -s "$(hostname) backup error" $email
	exit
}

# create temp folder (always do first as we trap exit to remove it)
tmpdir=$(mktemp -d -t backup-XXXXXXXX)
if [ ! -d $tmpdir ]; then ex "failed to make tmpdir"; fi
echo "tmpdir=$tmpdir"
cd $tmpdir
trap 'echo "removing $tmpdir"; rm -rf -- "$tmpdir"; echo "end $(date)"' EXIT

# check requirements
for x in "ccencrypt" "pigz" "aws" "pv"; do if [ -z "$(which $x)" ]; then ex "install $x"; fi; done
if [ ! -f "$keyfile" ]; then ex "key file $keyfile does not exist"; fi
if [ ! -r "$keyfile" ]; then ex "can't read key file"; fi

# create temp excludes file
touch $tmpdir/excludes
if [ ! -f $tmpdir/excludes ]; then ex "failed to make $tmpdir/excludes"; fi
for ex in "${excludes[@]}"; do echo "$ex" >> $tmpdir/excludes; done

# dump all databases
echo "dumping databases"
nice -n 19 mysqldump --defaults-extra-file=/etc/mysql/debian.cnf --all-databases -r ./databases.sql
if [ ! -f databases.sql ]; then ex "error dumping databases: file not found"; fi
if [ $(ls -al databases.sql | awk '{print $5}') -lt 100 ]; then ex "error dumping databases: file too small\n$(ls -alh databases.sql)"; fi
ls -alh databases.sql
files+=( "./databases.sql" )

# backup
datestr=$(date +%Y%m%d_%H%M%S_%Z)
echo "files=${files[@]@Q}"
echo "excludes=${excludes[@]@Q}"

if [ "$1" == "local" ]; then
	echo "local backup"
	file="/root/backup_$(hostname)_$datestr.tgz"
	echo "archiving to $file"
	nice -n 19 tar --exclude-from=$tmpdir/excludes -cf - "${files[@]}" 2>/dev/null | nice -n 19 pigz -9 | pv > $file
else
	echo "s3 backup"
	# archive, encrypt and pipe to s3
	s3file="s3://$bucket/backup_$(hostname)_$datestr.tgz.cpt"
	echo "archiving and encrypting while piping to $s3file ($storageclass)"
	nice -n 19 tar --exclude-from=$tmpdir/excludes -cf - "${files[@]}" 2>/dev/null | nice -n 19 pigz -9 | nice -n 19 ccencrypt -k $keyfile | pv | aws s3 cp - $s3file --storage-class=$storageclass

	# remove excess remote files (it's safe to have other host's backups and other things in the bucket)
	echo "removing excess remote files ($numkeep max)"
	aws s3 ls s3://$bucket/backup_$(hostname)_ | grep "\.cpt$" | sort | head -n -$numkeep | awk '{$1=$2=$3=""; print $0}' | sed 's/^[ \t]*//' | while read -r line; do aws s3 rm "s3://$bucket/$line"; done

	# verify upload exists and isn't too small indicating an archive error
	echo "verifying upload"
	a=$(aws s3 ls "$s3file" 2>/dev/null)
	if [ -z "$a" ]; then ex "upload not found"; fi
	if [ $(echo "$a" | awk '{print $3}') -lt 1000000 ]; then ex "upload too small\n$a"; fi
fi

# done
echo $a
echo "backup complete"